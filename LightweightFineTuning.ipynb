{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "021bb33a-14a0-4222-b186-3a32fc40d025",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659123e5-6a8c-4896-803e-1528027a5f97",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "We will be loading the dataset (dair-ai/emotion) and fine tune an existing model (microsoft/DialogRPT-updown) to detect emotion from the dataset.\n",
    "\n",
    "In this section we will load the dataset, tokenize the data for futher training.\n",
    "We will load the pre-trained foundation model and evaluate it's performance on the validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5314441-26a4-40ee-8a8e-befb8d6f553e",
   "metadata": {},
   "source": [
    "### Load the dataset and tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d2673f6-6861-4f16-9baf-3e2d5eb8d811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Set Loaded and Tokenized\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"dair-ai/emotion\")\n",
    "splits = [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialogRPT-updown\")\n",
    "\n",
    "# Let's use a lambda function to tokenize all the examples\n",
    "tokenized_dataset = {}\n",
    "for split in splits:\n",
    "    tokenized_dataset[split] = dataset[split].map(\n",
    "        lambda x: tokenizer(x[\"text\"], padding=True, truncation=True), batched=True\n",
    "    )\n",
    "\n",
    "print(\"Data Set Loaded and Tokenized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e48e0dd-4907-423d-86ed-e9ace229cb04",
   "metadata": {},
   "source": [
    "### Load the pre-trained foundational Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9fe6356-ab21-4a0a-8d39-3a340192beea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at microsoft/DialogRPT-updown and are newly initialized because the shapes did not match:\n",
      "- score.weight: found shape torch.Size([1, 1024]) in the checkpoint and torch.Size([6, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the foundation Model\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialogRPT-updown')\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/DialogRPT-updown\",\n",
    "    ignore_mismatched_sizes=True,\n",
    "    num_labels=6, # There are 20 possible labels (emoji's) for the text\n",
    ")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(\"Loaded the foundation Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8fe669-67cd-4108-b289-9207fcef4672",
   "metadata": {},
   "source": [
    "### Validating the existing model on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "003eeb3f-18bb-4aee-80cf-5f1387190805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/opt/homebrew/lib/python3.12/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "\n",
    "# The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\n",
    "# Read more about it here https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/model\",\n",
    "        # Set the learning rate\n",
    "        learning_rate=2e-5,\n",
    "        # Set the per device train batch size and eval batch size\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        # Evaluate and save the model after each epoch\n",
    "        save_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7164c29-8df3-4542-ade1-4187f5669b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model in Validation dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 03:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.7716858386993408,\n",
       " 'eval_accuracy': 0.2425,\n",
       " 'eval_runtime': 25.0379,\n",
       " 'eval_samples_per_second': 79.879,\n",
       " 'eval_steps_per_second': 19.97}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Evaluating the model in Validation dataset\")\n",
    "trainer.evaluate() #Evaluating the model on the Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f19c83b1-a31b-4387-b703-c6bfc0c63e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model in Test dataset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.7617275714874268,\n",
       " 'eval_accuracy': 0.25,\n",
       " 'eval_runtime': 19.4931,\n",
       " 'eval_samples_per_second': 102.601,\n",
       " 'eval_steps_per_second': 25.65}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Evaluating the model in Test dataset\")\n",
    "trainer.evaluate(tokenized_dataset[\"test\"]) #Eavaluating the model in the Test Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad83dad9-b98c-44fe-aee4-acf716aa1fb0",
   "metadata": {},
   "source": [
    "## Fine-tuning the foundational model using PEFT\n",
    "In this section, we will intialize the peft model based on the existing model.\n",
    "Once the model is loaded, we will initialize a trainer to train the model against our dataset.\n",
    "The fine tuned model, will be saved locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42b67d48-3935-4603-b3ac-33da4e4a65c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying fine-tuning by applying a PEFT model\n",
      "trainable params: 792,576 || all params: 355,621,888 || trainable%: 0.22287042129420334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/peft/tuners/lora/layer.py:1059: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print('Applying fine-tuning by applying a PEFT model')\n",
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\", inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.2)\n",
    "\n",
    "from peft import get_peft_model\n",
    "\n",
    "model_peft = get_peft_model(model, peft_config)\n",
    "model_peft.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37659665-b1cc-4a39-bf5c-ce62059520cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.12/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20000' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20000/20000 55:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.515600</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.896000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.450100</td>\n",
       "      <td>0.367630</td>\n",
       "      <td>0.911000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.417000</td>\n",
       "      <td>0.348243</td>\n",
       "      <td>0.914500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.429600</td>\n",
       "      <td>0.339444</td>\n",
       "      <td>0.917000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.384300</td>\n",
       "      <td>0.328587</td>\n",
       "      <td>0.916000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20000, training_loss=0.44645341720581055, metrics={'train_runtime': 3331.9462, 'train_samples_per_second': 24.01, 'train_steps_per_second': 6.002, 'total_flos': 1.0554954781753344e+16, 'train_loss': 0.44645341720581055, 'epoch': 5.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The HuggingFace Trainer class handles the training and eval loop for PyTorch for us.\n",
    "# Read more about it here https://huggingface.co/docs/transformers/main_classes/trainer\n",
    "trainer_peft = Trainer(\n",
    "    model=model_peft,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/dair-ai/models\",\n",
    "        # Set the learning rate\n",
    "        learning_rate=2e-5,\n",
    "        # Set the per device train batch size and eval batch size\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        # Evaluate and save the model after each epoch\n",
    "        save_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer_peft.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7632dfa-67c0-4a52-ae95-1f099c9818cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_peft.save_pretrained('lora-model') #Save the model in local directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a649bb-1696-4e95-8644-ec90a7497903",
   "metadata": {},
   "source": [
    "# Evaluating the trained model\n",
    "In this section we will load the trained model and evaluate against the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "092881c6-23db-459c-a63d-1a44fcc4af33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at microsoft/DialogRPT-updown and are newly initialized because the shapes did not match:\n",
      "- score.weight: found shape torch.Size([1, 1024]) in the checkpoint and torch.Size([6, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/homebrew/lib/python3.12/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3568718731403351,\n",
       " 'eval_accuracy': 0.9115,\n",
       " 'eval_runtime': 22.693,\n",
       " 'eval_samples_per_second': 88.133,\n",
       " 'eval_steps_per_second': 22.033}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained model from local directory.\n",
    "from peft import AutoPeftModelForSequenceClassification\n",
    "lora_model = AutoPeftModelForSequenceClassification.from_pretrained(\n",
    "    \"lora-model\",\n",
    "    num_labels=6,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Initialize the trainer with the new model\n",
    "trainer_lora = Trainer(\n",
    "    model=lora_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/models\",\n",
    "        # Set the learning rate\n",
    "        learning_rate=2e-5,\n",
    "        # Set the per device train batch size and eval batch size\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        # Evaluate and save the model after each epoch\n",
    "        save_strategy=\"epoch\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Evaluate the model against the test dataset.\n",
    "trainer_lora.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8adf54-4226-438f-9b27-182de9f9acf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
